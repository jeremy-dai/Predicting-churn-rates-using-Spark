{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmos2spark, os\n",
    "# @hidden_cell\n",
    "\n",
    "if os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n",
    "    endpoint_da32d840b0c6496da7e4b4ae965e36bf = 'https://s3.eu-geo.objectstorage.softlayer.net'\n",
    "else:\n",
    "    endpoint_da32d840b0c6496da7e4b4ae965e36bf = 'https://s3.eu-geo.objectstorage.service.networklayer.com'\n",
    "\n",
    "credentials = {\n",
    "    'endpoint': endpoint_da32d840b0c6496da7e4b4ae965e36bf,\n",
    "    'service_id': 'iam-ServiceId-534b61b5-307d-474a-8ba4-deebac0187b5',\n",
    "    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'api_key': 'XiBbMTH05VLVP_M-y0DgAdgBNzIrmYT8Mmqr8kX51gzV'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_da32d840b0c6496da7e4b4ae965e36bf_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n",
    "# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n",
    "# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n",
    "\n",
    "df = spark.read.json(cos.url('medium-sparkify-event-data.json', 'churnspark-donotdelete-pr-pty3wun15oklrj'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "#from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete empty userIds\n",
    "df = df.dropna(how = 'any', subset = ['userId', 'sessionId'])\n",
    "df = df.filter(df.userId != '')\n",
    "\n",
    "### delete useless/sensitve information\n",
    "### i.e. songs, names\n",
    "df = df.select('auth', 'gender', 'itemInSession',\\\n",
    "     'level', 'location', 'method', 'page', 'registration',\\\n",
    "       'sessionId', 'status', 'ts', 'userAgent', 'userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users cancelled their paid accounts\n",
    "flag_cancel_event = F.udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "df = df.withColumn('cancel', flag_cancel_event('page'))\n",
    "\n",
    "windowval = Window.partitionBy('userId').orderBy(F.desc('ts')).rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df = df.withColumn('churn', F.sum('cancel').over(windowval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Frequency: based on sessions\n",
    "df_freq = df.groupby('userId').agg({'ts': 'max',\\\n",
    "                                    'registration': 'max',\\\n",
    "                                    'sessionId':'count'})\n",
    "\n",
    "df_freq = df_freq.withColumn('days', (F.col('max(ts)') - F.col('max(registration)'))/(1000*60*60*24))\\\n",
    "        .withColumn('freq', F.col('count(sessionId)')/F.col('days'))\\\n",
    "        .select('userId', 'freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many songs do users listen to on average between visiting the home page? \n",
    "ifhome = F.udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "\n",
    "user_window = Window \\\n",
    "    .partitionBy('userId') \\\n",
    "    .orderBy(F.desc('ts')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "cusum = df.filter((df.page == 'NextSong') | (df.page == 'Home')) \\\n",
    "    .select('userId', 'page', 'ts') \\\n",
    "    .withColumn('homevisit', ifhome(F.col('page'))) \\\n",
    "    .withColumn('period', F.sum('homevisit').over(user_window))\n",
    "\n",
    "df_songs = cusum.filter((cusum.page == 'NextSong')) \\\n",
    "    .groupBy('userID', 'period') \\\n",
    "    .agg({'period':'count'})\\\n",
    "    .groupBy('userID') \\\n",
    "    .agg(F.avg('count(period)').alias(\"Songs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time per session\n",
    "df_time = df.groupby(\"userId\", \"sessionId\")\\\n",
    "    .agg((F.max(df.ts)-F.min(df.ts)).alias(\"duration\"))\\\n",
    "    .groupby(\"userId\").agg(F.avg(F.col('duration')).alias(\"time_session\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage time in different page type \n",
    "time_page_each = df.groupby(\"userId\")\\\n",
    ".pivot('page').agg(F.count('page'))\\\n",
    ".fillna(0)\n",
    "\n",
    "cols = time_page_each.columns\n",
    "\n",
    "time_page_total = df.groupby(\"userId\")\\\n",
    ".agg(F.count('page'))\n",
    "\n",
    "time_page_each = time_page_each.join(time_page_total, ['userId'], how='left')\n",
    "\n",
    "for col in cols[1:]: # get rid of 'userId' in the cols\n",
    "    time_page_each =  time_page_each\\\n",
    "        .withColumn(col, F.col(col)/F.col('count(page)'))\n",
    "\n",
    "df_page = time_page_each.drop('count(page)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender\n",
    "df_gender = df.select('userId', 'gender').dropDuplicates()\n",
    "\n",
    "int_gender = F.udf(lambda x: 1 if x == \"F\" else 0, IntegerType())\n",
    "\n",
    "df_gender = df_gender.withColumn('gender', int_gender('gender'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ever paid\n",
    "df_level = df.select('userId', 'level').dropDuplicates()\n",
    "\n",
    "int_level = F.udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
    "\n",
    "df_level = df_level.withColumn('level', int_level('level'))\\\n",
    "    .groupby('userId').agg(F.max('level').alias('if_paid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# churn\n",
    "df_churn = df.select('userId', 'churn').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create full dataset\n",
    "full_data = df_churn\n",
    "feature_list = [df_freq, df_songs, df_time, df_page, df_gender, df_level]\n",
    "\n",
    "for f in feature_list:\n",
    "    full_data = full_data.join(f, ['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the features\n",
    "assembler = VectorAssembler(inputCols=full_data.columns[2:], outputCol=\"NumFeatures\")\n",
    "full_data = assembler.transform(full_data) \n",
    "\n",
    "# standarlize the features\n",
    "scaler = StandardScaler(inputCol='NumFeatures', outputCol='features', withMean=True, withStd=True)\n",
    "scalerModel = scaler.fit(full_data)\n",
    "full_data = scalerModel.transform(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data.select(F.col('churn').alias(\"label\"), 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, features: vector]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = full_data.randomSplit([0.9, 0.1], seed=42)\n",
    "train.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.3])\\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9135136930091392,\n",
       " 0.9351917142850934,\n",
       " 0.9602657767482108,\n",
       " 0.9648186183109246]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8277777777777777"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalutaor = BinaryClassificationEvaluator()\n",
    "evalutaor.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.4053, 0.0, 0.2039, -0.03, -0.0087, -0.1134, 2.3632, 2.3632, 0.4971, -0.0193, 0.0289, -0.1068, -0.1357, -0.3546, 0.5034, -0.1207, 0.1394, -0.1241, -0.0, 0.1258, -0.1653, 0.0, 0.0, 0.5197])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are in the order of df_freq, df_songs, df_time, df_page, df_gender, df_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  0]\n",
      " [ 3  3]]\n",
      "precision: 1.0\n",
      "recall: 0.5\n",
      "f1 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(results.select('label').collect(), results.select('prediction').collect())\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp) \n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 with Spark",
   "language": "python3",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
