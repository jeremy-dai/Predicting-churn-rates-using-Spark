{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20210318004239-0000\n",
      "KERNEL_ID = e9071b9e-0c90-4d65-8dc6-b7f377d802c5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martin Orford', auth='Logged In', firstName='Joseph', gender='M', itemInSession=20, lastName='Morales', length=597.55057, level='free', location='Corpus Christi, TX', method='PUT', page='NextSong', registration=1532063507000, sessionId=292, song='Grand Designs', status=200, ts=1538352011000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='293'),\n",
       " Row(artist=\"John Brown's Body\", auth='Logged In', firstName='Sawyer', gender='M', itemInSession=74, lastName='Larson', length=380.21179, level='free', location='Houston-The Woodlands-Sugar Land, TX', method='PUT', page='NextSong', registration=1538069638000, sessionId=97, song='Bulls', status=200, ts=1538352025000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='98'),\n",
       " Row(artist='Afroman', auth='Logged In', firstName='Maverick', gender='M', itemInSession=184, lastName='Santiago', length=202.37016, level='paid', location='Orlando-Kissimmee-Sanford, FL', method='PUT', page='NextSong', registration=1535953455000, sessionId=178, song='Because I Got High', status=200, ts=1538352118000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='179'),\n",
       " Row(artist=None, auth='Logged In', firstName='Maverick', gender='M', itemInSession=185, lastName='Santiago', length=None, level='paid', location='Orlando-Kissimmee-Sanford, FL', method='PUT', page='Logout', registration=1535953455000, sessionId=178, song=None, status=307, ts=1538352119000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='179'),\n",
       " Row(artist='Lily Allen', auth='Logged In', firstName='Gianna', gender='F', itemInSession=22, lastName='Campos', length=194.53342, level='paid', location='Mobile, AL', method='PUT', page='NextSong', registration=1535931018000, sessionId=245, song='Smile (Radio Edit)', status=200, ts=1538352124000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='246')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ibmos2spark, os\n",
    "# @hidden_cell\n",
    "\n",
    "if os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n",
    "    endpoint_da32d840b0c6496da7e4b4ae965e36bf = 'https://s3.eu-geo.objectstorage.softlayer.net'\n",
    "else:\n",
    "    endpoint_da32d840b0c6496da7e4b4ae965e36bf = 'https://s3.eu-geo.objectstorage.service.networklayer.com'\n",
    "\n",
    "credentials = {\n",
    "    'endpoint': endpoint_da32d840b0c6496da7e4b4ae965e36bf,\n",
    "    'service_id': 'iam-ServiceId-534b61b5-307d-474a-8ba4-deebac0187b5',\n",
    "    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'api_key': 'XiBbMTH05VLVP_M-y0DgAdgBNzIrmYT8Mmqr8kX51gzV'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_da32d840b0c6496da7e4b4ae965e36bf_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n",
    "# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n",
    "# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n",
    "\n",
    "df = spark.read.json(cos.url('medium-sparkify-event-data.json', 'churnspark-donotdelete-pr-pty3wun15oklrj'))\n",
    "df.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "#from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete empty userIds\n",
    "df = df.dropna(how = 'any', subset = ['userId', 'sessionId'])\n",
    "df = df.filter(df.userId != '')\n",
    "\n",
    "### delete useless/sensitve information\n",
    "### i.e. songs, names\n",
    "df = df.select('auth', 'gender', 'itemInSession',\\\n",
    "     'level', 'location', 'method', 'page', 'registration',\\\n",
    "       'sessionId', 'status', 'ts', 'userAgent', 'userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users cancelled their paid accounts\n",
    "flag_cancel_event = F.udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "df = df.withColumn('cancel', flag_cancel_event('page'))\n",
    "\n",
    "windowval = Window.partitionBy('userId').orderBy(F.desc('ts')).rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df = df.withColumn('churn', F.sum('cancel').over(windowval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Frequency: based on sessions\n",
    "df_freq = df.groupby('userId').agg({'ts': 'max',\\\n",
    "                                    'registration': 'max',\\\n",
    "                                    'sessionId':'count'})\n",
    "\n",
    "df_freq = df_freq.withColumn('days', (F.col('max(ts)') - F.col('max(registration)'))/(1000*60*60*24))\\\n",
    "        .withColumn('freq', F.col('count(sessionId)')/F.col('days'))\\\n",
    "        .select('userId', 'freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many songs do users listen to on average between visiting the home page? \n",
    "ifhome = F.udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "\n",
    "user_window = Window \\\n",
    "    .partitionBy('userId') \\\n",
    "    .orderBy(F.desc('ts')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "cusum = df.filter((df.page == 'NextSong') | (df.page == 'Home')) \\\n",
    "    .select('userId', 'page', 'ts') \\\n",
    "    .withColumn('homevisit', ifhome(F.col('page'))) \\\n",
    "    .withColumn('period', F.sum('homevisit').over(user_window))\n",
    "\n",
    "df_songs = cusum.filter((cusum.page == 'NextSong')) \\\n",
    "    .groupBy('userID', 'period') \\\n",
    "    .agg({'period':'count'})\\\n",
    "    .groupBy('userID') \\\n",
    "    .agg(F.avg('count(period)').alias(\"Songs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time per session\n",
    "df_time = df.groupby(\"userId\", \"sessionId\")\\\n",
    "    .agg((F.max(df.ts)-F.min(df.ts)).alias(\"duration\"))\\\n",
    "    .groupby(\"userId\").agg(F.avg(F.col('duration')).alias(\"time_session\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage time in different page type \n",
    "time_page_each = df.groupby(\"userId\")\\\n",
    ".pivot('page').agg(F.count('page'))\\\n",
    ".fillna(0)\n",
    "\n",
    "cols = time_page_each.columns\n",
    "\n",
    "time_page_total = df.groupby(\"userId\")\\\n",
    ".agg(F.count('page'))\n",
    "\n",
    "time_page_each = time_page_each.join(time_page_total, ['userId'], how='left')\n",
    "\n",
    "for col in cols[1:]: # get rid of 'userId' in the cols\n",
    "    time_page_each =  time_page_each\\\n",
    "        .withColumn(col, F.col(col)/F.col('count(page)'))\n",
    "\n",
    "df_page = time_page_each.drop('count(page)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender\n",
    "df_gender = df.select('userId', 'gender').dropDuplicates()\n",
    "\n",
    "int_gender = F.udf(lambda x: 1 if x == \"F\" else 0, IntegerType())\n",
    "\n",
    "df_gender = df_gender.withColumn('gender', int_gender('gender'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ever paid\n",
    "df_level = df.select('userId', 'level').dropDuplicates()\n",
    "\n",
    "int_level = F.udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
    "\n",
    "df_level = df_level.withColumn('level', int_level('level'))\\\n",
    "    .groupby('userId').agg(F.max('level').alias('if_paid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(userId='100010', churn=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# churn\n",
    "df_churn = df.select('userId', 'churn').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create full dataset\n",
    "full_data = df_churn\n",
    "feature_list = [df_freq, df_songs, df_time, df_page, df_gender, df_level]\n",
    "\n",
    "for f in feature_list:\n",
    "    full_data = full_data.join(f, ['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the features\n",
    "assembler = VectorAssembler(inputCols=full_data.columns[2:], outputCol=\"NumFeatures\")\n",
    "full_data = assembler.transform(full_data) \n",
    "\n",
    "# standarlize the features\n",
    "scaler = StandardScaler(inputCol='NumFeatures', outputCol='features', withMean=True, withStd=True)\n",
    "scalerModel = scaler.fit(full_data)\n",
    "full_data = scalerModel.transform(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data.select(F.col('churn').alias(\"label\"), 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, features: vector]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = full_data.randomSplit([0.9, 0.1], seed=42)\n",
    "train.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8729759435073445, 0.9240122377102274]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  0]\n",
      " [ 3  3]]\n",
      "precision: 1.0\n",
      "recall: 0.5\n",
      "f1 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(results.select('label').collect(), results.select('prediction').collect())\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp) \n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 with Spark",
   "language": "python3",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
